<!DOCTYPE html>
<html>
  <head>
    <title>Code Implementation - MARS Robot driving</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="stylesheet" href="assets/css/main.css" />
    <link
      href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css"
      rel="stylesheet"
    />
    <style>
      /* Base styles for code wrappers (fallback) */
      .code-wrapper {
        width: 90%;
        max-width: 800px;
        margin: 1rem auto;
        text-align: left;
        background-color: #272822;
        border-radius: 6px;
        box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1);
        overflow-x: auto;
        position: relative;
      }

      /* Override Prism defaults to prevent double padding/margin */
      .code-wrapper pre[class*="language-"] {
        margin: 0 !important;
        padding: 1.5rem !important;
        background: transparent !important;
        border: none !important;
        box-shadow: none !important;
        overflow: visible !important;
        width: 100%;
        float: none;
      }

      /* Font styling for code */
      code[class*="language-"],
      pre[class*="language-"] {
        font-family: "Consolas", "Monaco", "Courier New", monospace !important;
        font-size: 0.9em !important;
        line-height: 1.6 !important;
        text-shadow: none !important;
      }

      /* Formatting for the explanatory text */
      .align-center p,
      .align-center ul,
      .align-center ol {
        text-align: left;
        max-width: 800px;
        margin-left: auto;
        margin-right: auto;
        font-size: 1.1em;
        line-height: 1.8;
      }

      .align-center h2,
      .align-center h3,
      .align-center h4 {
        text-align: center;
      }
    </style>
  </head>

  <body>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>

    <div id="wrapper" class="divided">
      <section
        class="banner style1 orient-center content-align-center image-position-center onload-image-fade-in onload-content-fade-right"
      >
        <div class="content">
          <h1>Driving</h1>
          <p class="major">
            This page describes how the robot autonomously detects the target
            object, estimates its position, plans a smooth trajectory, and
            drives precisely to a stopping point. The system unifies perception
            and motion into a single, repeatable workflow.
          </p>
          <p class="major">
            For this task, LiDAR-based perception was intentionally omitted.
            Since the environment is planar and the objective is limited to
            detecting a single object, a 2D vision-based approach provided
            sufficient accuracy while reducing complexity. All motion planning
            is performed in a world-fixed frame rather than the robot’s local
            frame, allowing for more stable and predictable behavior.
          </p>
          <ul class="actions vertical">
            <li>
              <a
                href="index.html#detect"
                class="button big wide smooth-scroll-middle"
                >Return to Main Page</a
              >
            </li>
          </ul>
        </div>
        <div class="image">
          <img src="images/spotlight01.jpg" alt="" />
        </div>
      </section>

      <section class="wrapper style1 align-center" id="detection">
        <div class="inner">
          <h2>1. Detection and Image Processing</h2>
          <p>
            The perception pipeline begins with processing images from the
            onboard camera. The raw RGB image is converted to grayscale to
            reduce noise, and a white intensity threshold is applied to isolate
            the target.
          </p>
          <p>
            We apply morphological operations to refine the binary image.
            <strong>Erosion followed by dilation</strong> removes noise, while
            <strong>dilation followed by erosion</strong> fills gaps in the
            object's silhouette. Finally, plane filtering rejects detections
            outside the expected ground plane.
          </p>
        </div>

        <div
          style="
            width: 70%;
            max-width: 100%;
            margin: 2rem auto;
            text-align: left;
            background-color: #272822;
            border-radius: 6px;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1);
            overflow-x: auto;
            position: relative;
          "
        >
          <pre
            style="
              margin: 0 !important;
              padding: 1.5rem !important;
              background: transparent !important;
              border: none !important;
              width: 100%;
            "
          ><code class="language-python"># From detect_object.py

def detect_white_object(self, image, camera_params, camera_name="unknown"):
    # Convert to grayscale
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    
    # Threshold to find white objects
    _, binary = cv2.threshold(gray, self.white_threshold, 255, cv2.THRESH_BINARY)
    
    # FILTER: Masking (Base camera ignores upper half of image)
    if camera_name == "BASE":
        mask = np.zeros_like(binary)
        mask[h_img//2:, :] = 255  # Only bottom half
        binary = cv2.bitwise_and(binary, mask)
    
    # Morphological operations to remove noise
    kernel = np.ones((5, 5), np.uint8)
    binary = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)
    binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)</code></pre>
        </div>
      </section>

      <section class="wrapper style1 align-center" id="coordinates">
        <div class="inner">
          <h3>2. Identifying Object Coordinates</h3>
          <p>
            The object's centroid is computed using
            <strong>image moments</strong>, which is more robust than a bounding
            box center. Depth is estimated using
            <strong>similar triangles</strong> based on the object's known
            physical size. We project the 2D pixel coordinates into a 3D
            position vector (x, y, z) in the camera frame.
          </p>
        </div>

        <div
          style="
            width: 70%;
            max-width: 100%;
            margin: 2rem auto;
            text-align: left;
            background-color: #272822;
            border-radius: 6px;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1);
            overflow-x: auto;
            position: relative;
          "
        >
          <pre
            style="
              margin: 0 !important;
              padding: 1.5rem !important;
              background: transparent !important;
              border: none !important;
              width: 100%;
            "
          ><code class="language-python"># From detect_object.py: Depth & 3D Projection

# Calculate Centroid using Moments
M = cv2.moments(largest_contour)
cx = int(M['m10'] / M['m00'])
cy = int(M['m01'] / M['m00'])

# Distance Estimation (Similar Triangles)
pixel_size = max(w, h)
f = (camera_params['fx'] + camera_params['fy']) / 2.0
distance = (self.object_real_size * f) / pixel_size

# Resulting 3D point (x, y, z) in meters
point_3d = np.array([x_norm * distance, y_norm * distance, distance])</code></pre>
        </div>
      </section>

      <section class="wrapper style1 align-center" id="localization">
        <div class="inner">
          <h3>3. Coordinate Frames & Localization</h3>
          <p>
            To plan motion consistently, all positions are transformed into a
            common reference frame. We use <strong>tf2</strong> to convert poses
            from the robot’s body frame (<code>base_link</code>) into the
            world-fixed odometry frame (<code>odom</code>). This allows us to
            track absolute position and orientation (yaw) for stable path
            planning.
          </p>
        </div>

        <div
          style="
            width: 70%;
            max-width: 100%;
            margin: 2rem auto;
            text-align: left;
            background-color: #272822;
            border-radius: 6px;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1);
            overflow-x: auto;
            position: relative;
          "
        >
          <pre
            style="
              margin: 0 !important;
              padding: 1.5rem !important;
              background: transparent !important;
              border: none !important;
              width: 100%;
            "
          ><code class="language-python"># From move_to_object.py: TF2 & Localization

# Get transform from Odom (world) to Base Link (robot)
trans = self.tf_buffer.lookup_transform('odom', 'base_link', rclpy.time.Time(), timeout=rclpy.duration.Duration(seconds=0.1))

# Extract Position and Yaw
x = trans.transform.translation.x
y = trans.transform.translation.y
roll, pitch, yaw = euler.quat2euler([q.w, q.x, q.y, q.z])</code></pre>
        </div>
      </section>

      <section class="wrapper style1 align-center" id="trajectory">
        <div class="inner">
          <h3>4. Trajectory Design</h3>
          <p>
            Rather than driving directly toward the target, the robot follows a
            <strong>cubic Bézier curve</strong>. This generates smooth curvature
            and continuous heading changes, resulting in mechanically gentle
            motion compared to simple point-to-point navigation.
          </p>
          <p>
            The trajectory is sampled into a sequence of waypoints, each
            containing a position and heading. To ensure safe behavior, the
            trajectory is generated up to a fixed offset from the object,
            preventing collision while placing the object in the workspace.
          </p>
        </div>

        <div
          style="
            width: 70%;
            max-width: 100%;
            margin: 2rem auto;
            text-align: left;
            background-color: #272822;
            border-radius: 6px;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1);
            overflow-x: auto;
            position: relative;
          "
        >
          <pre
            style="
              margin: 0 !important;
              padding: 1.5rem !important;
              background: transparent !important;
              border: none !important;
              width: 100%;
            "
          ><code class="language-python"># From move_to_object.py: Bézier Curve Generation

def bezier_curve(self, p0, p1, p2, p3, t):
    """Cubic Bézier interpolation between 4 control points."""
    return (1 - t)**3 * p0 + 3*(1 - t)**2*t*p1 + 3*(1 - t)*t**2*p2 + t**3*p3

def generate_bezier_waypoints(self, x1, y1, theta1, x2, y2, theta2):
    # Control points offset along current heading direction
    d1 = np.array([np.cos(theta1), np.sin(theta1)])
    c1 = np.array([x1, y1]) + self.bezier_offset * d1
    
    # Sample points along the curve
    pts = [self.bezier_curve(..., t) for t in t_vals]
    return pts</code></pre>
        </div>
      </section>

      <section class="wrapper style1 align-center" id="control">
        <div class="inner">
          <h3>5. Steering and Velocity Control</h3>
          <p>
            A closed-loop controller runs at 20Hz. We use a
            <strong>Low Pass Filter</strong> to smooth angular velocity commands
            and a <strong>Deadband</strong> to ignore micro-errors when aligned.
            Linear velocity is dynamically adjusted based on turning sharpness
            (slowing down for corners) and proximity to the target.
          </p>
        </div>

        <div
          style="
            width: 70%;
            max-width: 100%;
            margin: 2rem auto;
            text-align: left;
            background-color: #272822;
            border-radius: 6px;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1);
            overflow-x: auto;
            position: relative;
          "
        >
          <pre
            style="
              margin: 0 !important;
              padding: 1.5rem !important;
              background: transparent !important;
              border: none !important;
              width: 100%;
            "
          ><code class="language-python"># From move_to_object.py: Control Logic

# 1. Low-Pass Filter (Exponential Moving Average)
self.filtered_angular_vel = (self.angular_filter_alpha * raw_angular_vel + 
                              (1 - self.angular_filter_alpha) * self.filtered_angular_vel)

# 2. Deadband (Noise Reduction)
if abs(angle_error) < self.angular_deadband:
    angle_error = 0.0

# 3. Dynamic Speed Adjustment
turn_factor = max(0.4, 1.0 - abs(angle_error) / (math.pi / 2))
approach_factor = min(1.0, dist / 0.2)
linear_vel = self.linear_speed * turn_factor * approach_factor

# Prevent stalling
linear_vel = max(0.05, linear_vel)</code></pre>
        </div>
      </section>

      <footer class="wrapper style1 align-center">
        <div class="inner">
          <p>&copy; Intro to Robotics Project.</p>
        </div>
      </footer>
    </div>

    <script src="assets/js/jquery.min.js"></script>
    <script src="assets/js/jquery.scrollex.min.js"></script>
    <script src="assets/js/jquery.scrolly.min.js"></script>
    <script src="assets/js/skel.min.js"></script>
    <script src="assets/js/util.js"></script>
    <script src="assets/js/main.js"></script>
  </body>
</html>
